{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b672c-b110-4319-bfe8-f8525d3c61de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Bidirectional, LSTM, Dense\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.compat.v1.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=0.05)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "929d2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nans(dataset):\n",
    " \n",
    "\n",
    " indices = np.where(np.isnan(dataset))\n",
    " nan_indices = list(zip(indices))\n",
    "\n",
    "\n",
    " for i in range (len(indices[0])):\n",
    "    dataset[indices[0][i]][indices[1][i]] = np.nanmean(dataset[:,indices[1][i]])\n",
    " return dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d33e871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function reads , stores and returns all data as numpy arrays\n",
    "def read_all_data():\n",
    "  #reading the training data\n",
    "  df1 = pd.read_csv(\"C:\\\\Users\\\\91950\\\\OneDrive\\\\Desktop\\\\test_data_CNN_filter\\\\Zdata.csv\")\n",
    "  time_series_data = df1[['f1','f2','f3','f4','f5','f6','f7','f8','f9','f10','f11','f12','f13','f14','f15','f16','f17','f18','f19','f20','f21','f22','f23','f24','f25','f26','f27','f28','f29','f30','f31','f32','f33','f34','f35','f36','f37','f38','f39','f40']].values\n",
    "  time_series_data_1 = replace_nans(time_series_data)\n",
    "\n",
    "  #reading  fdi data\n",
    "  df2 = pd.read_csv(\"C:\\\\Users\\\\91950\\\\OneDrive\\\\Desktop\\\\test_data_CNN_filter\\\\Z_FDI.csv\")\n",
    "  fdi_test_data = df2[['f1','f2','f3','f4','f5','f6','f7','f8','f9','f10','f11','f12','f13','f14','f15','f16','f17','f18','f19','f20','f21','f22','f23','f24','f25','f26','f27','f28','f29','f30','f31','f32','f33','f34','f35','f36','f37','f38','f39','f40']].values\n",
    "  fdi_test_data_1= replace_nans(fdi_test_data)\n",
    "\n",
    "  #reading trip_merge  data\n",
    "  df3 = pd.read_csv(\"C:\\\\Users\\\\91950\\\\OneDrive\\\\Desktop\\\\test_data_CNN_filter\\\\Z_trip_merge.csv\")\n",
    "  trip_test_data = df3[['f1','f2','f3','f4','f5','f6','f7','f8','f9','f10','f11','f12','f13','f14','f15','f16','f17','f18','f19','f20','f21','f22','f23','f24','f25','f26','f27','f28','f29','f30','f31','f32','f33','f34','f35','f36','f37','f38','f39','f40']].values\n",
    "  trip_test_data_1 = replace_nans(trip_test_data)\n",
    "\n",
    "  #reading fault data\n",
    "  df4 = pd.read_csv(\"C:\\\\Users\\\\91950\\\\OneDrive\\\\Desktop\\\\test_data_CNN_filter\\\\Z_F_M2.csv\")\n",
    "  fault_test_data = df4[['f1','f2','f3','f4','f5','f6','f7','f8','f9','f10','f11','f12','f13','f14','f15','f16','f17','f18','f19','f20','f21','f22','f23','f24','f25','f26','f27','f28','f29','f30','f31','f32','f33','f34','f35','f36','f37','f38','f39','f40']].values\n",
    "  fault_test_data_1 = replace_nans(fault_test_data)\n",
    "\n",
    "  return time_series_data_1, fdi_test_data_1, trip_test_data_1, fault_test_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b4defd-082a-455d-8a42-d59e6ec8eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data, fdi_test_data, trip_test_data, fault_test_data = read_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f43b9a-d745-45e8-8a5b-d063958e30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging and shuffling steady state data and fault data \n",
    "merged_time_series_data = np.array((time_series_data.shape[0]+fault_test_data.shape[0],time_series_data.shape[1]))\n",
    "merged_time_series_data = np.concatenate((time_series_data,fault_test_data))\n",
    "np.random.shuffle(merged_time_series_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bff8e-5eea-413e-bbbd-f943997da07f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " #normalizing all data\n",
    "scalers = {}\n",
    "for i in range(merged_time_series_data.shape[1]):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    merged_time_series_data[:, i] = scaler.fit_transform(merged_time_series_data[:, i].reshape(-1, 1)).flatten()\n",
    "    scalers[f'feature{i + 1}'] = scaler\n",
    "\n",
    "  #changing the data into 1 d array to be fed into input layer\n",
    "new_time_series_data = merged_time_series_data.flatten()\n",
    "new_time_series_data = np.reshape(new_time_series_data,(new_time_series_data.shape[0],1))\n",
    "#splitting given data into training ,validation and testing data\n",
    "train_data, temp_data = train_test_split(new_time_series_data, test_size=0.2, shuffle=False)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "#specifying dimensions of input\n",
    "sequence_length = len(train_data)\n",
    "# Creating the input layer\n",
    "input_layer = Input(shape=(sequence_length, 1))\n",
    "# Building  the encoder using Bidirectional LSTM\n",
    "encoder_lstm1 = Bidirectional(LSTM(128, return_sequences=True))(input_layer)\n",
    "encoder_lstm2 = Bidirectional(LSTM(64,return_sequences = True))(encoder_lstm1)\n",
    "# Building the decoder using Bidirectional LSTM\n",
    "decoder_lstm1 = Bidirectional(LSTM(64, return_sequences=True))(encoder_lstm2)\n",
    "decoder_lstm2 = Bidirectional(LSTM(128, return_sequences=True))(decoder_lstm1)\n",
    "\n",
    "# Adding a dense layer for mapping back to original dimension\n",
    "input_dim = 1\n",
    "decoded_output = Dense(input_dim, activation ='sigmoid')(decoder_lstm2)\n",
    "# Creating the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded_output)\n",
    "# Compiling the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#summary of the autoencoder model\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f88c63-177e-4f20-b28d-5bf22e6c8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "autoencoder.fit(train_data[0:100000], train_data[0:100000], epochs=14, batch_size=32, validation_split=0.2)\n",
    "print(\"autoencoder model has been trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3731006e-2e9b-4504-b554-33e2ed68980e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2e934-bda3-4f27-ae67-30403b342f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking how the model performs on validation set to see if there's overfitting or underfitting\n",
    "predicted_validation = autoencoder.predict(val_data[0:20000])\n",
    "reconstruction_error_val = np.mean(np.square(predicted_validation - val_data[0:20000]), axis =1)\n",
    "\n",
    "\n",
    "print(reconstruction_error_val)\n",
    "print(np.mean(np.square(reconstruction_error_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f78143bf-31d0-4ca7-ac80-aa456f9f6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function that takes in test data and gives out reconstruction error of each data point in shape of original test data\n",
    "def reconstruction_errors(test_dataset, num_features):\n",
    "\n",
    "    #normalizing test dataset\n",
    "    scalers = {}\n",
    "    for i in range(test_dataset.shape[1]):\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        test_dataset[:, i] = scaler.fit_transform(test_dataset[:, i].reshape(-1, 1)).flatten()\n",
    "        scalers[f'feature{i + 1}'] = scaler\n",
    "    \n",
    "    #converting the normalized test dataset into 1d array to be fed into the autoencoder model\n",
    "    flattened_test_dataset = test_dataset.flatten()\n",
    "    flattened_test_dataset = np.reshape(flattened_test_dataset,(flattened_test_dataset.shape[0],1))\n",
    "\n",
    "    #making predictions\n",
    "    predicted_dataset = autoencoder.predict(flattened_test_dataset)\n",
    "    #reshaping flattneed predicted dataset into original dimension as given in test dataset\n",
    "    predicted_dataset = np.reshape(predicted_dataset,(int(predicted_dataset.shape[0] /num_features),int(num_features)))\n",
    "\n",
    "    #defining reconstruction error matrix\n",
    "    reconstruction_error = np.zeros_like(test_dataset, dtype=float)\n",
    "\n",
    "    #calculating reconstruction error in each datapoint\n",
    "    for k in range (test_dataset.shape[0]):\n",
    "        for j in range(test_dataset.shape[1]):\n",
    "            reconstruction_error[k][j] = np.mean(np.square(predicted_dataset[k][j] - test_dataset[k][j]))\n",
    "    return reconstruction_error\n",
    "    return np.mean(np.square(reconstruction_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad72fba3-bb9b-4460-b3cc-bb1a20893268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to calculate anomalies based on set  thresholds\n",
    "def anomalies(reconstruction_error, threshold):\n",
    "    num_anomalies = 0\n",
    "    #reconstruction_error = reconstruction_errors(\"test_data\",num_features)\n",
    "    for i in range (reconstruction_error.shape[0]):\n",
    "        for j in range (reconstruction_error.shape[1]):\n",
    "            if(reconstruction_error[i][j] >threshold):\n",
    "              num_anomalies = num_anomalies+1\n",
    "              print(i+1,j+1)\n",
    "    print(num_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f34f69-44d8-4dd9-9024-bfc296cb5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 40\n",
    "#calling functions to make predictions on test data (3 test dataset) and return reconstruction errors\n",
    "\n",
    "\n",
    "recons_error_trip  = reconstruction_errors(trip_test_data,(num_features))\n",
    "print(\"trip data reconstruction error \", np.mean(np.square(recons_error_trip)))\n",
    "recons_error_fault = reconstruction_errors(fault_test_data[0:20000],(num_features))\n",
    "print(\" fault data reconstruction error \", np.mean(np.square(recons_error_fault)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_error_fdi   = reconstruction_errors(fdi_test_data,int(num_features))\n",
    "print(\"fdi reconstruction error \", np.mean(np.square(recons_error_fdi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7fe5097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output the reconstruction errors , helpful in debugging and understanding \n",
    "#with open('output.txt', 'w') as f:\n",
    "#    f.write(np.array2string(recons_error_fdi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f0be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting threshold\n",
    "threshold = 0.1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c18e2d-fc14-416d-9fc6-8d4e3699463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling functions to find and print indexes of detected anomalies\n",
    "anomalies(recons_error_fault,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861cd57b-eb8e-4ca8-b141-1f7e34cbb92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies(recons_error_fdi,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99d46e-d495-4d27-8ec5-29de48330651",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies(recons_error_trip,threshold)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
